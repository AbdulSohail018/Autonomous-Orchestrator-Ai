name: Docker Build & Push

on:
  push:
    branches: [ main ]
    tags: [ 'v*' ]
  pull_request:
    branches: [ main ]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  build-producer:
    name: Build Producer Image
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Log in to Container Registry
      if: github.event_name != 'pull_request'
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}

    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/producer
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=semver,pattern={{version}}
          type=semver,pattern={{major}}.{{minor}}
          type=sha,prefix={{branch}}-

    - name: Build and push Docker image
      uses: docker/build-push-action@v5
      with:
        context: .
        file: ./Dockerfile.producer
        platforms: linux/amd64,linux/arm64
        push: ${{ github.event_name != 'pull_request' }}
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

  build-spark:
    name: Build Spark Job Image
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Create Spark Dockerfile
      run: |
        cat > Dockerfile.spark << 'EOF'
        FROM bitnami/spark:3.4.0

        USER root

        # Install Python dependencies
        COPY spark/requirements.txt /tmp/requirements.txt
        RUN pip install --no-cache-dir -r /tmp/requirements.txt

        # Copy Spark job files
        COPY spark/ /opt/spark/jobs/
        COPY airflow/include/ /opt/spark/config/

        # Set permissions
        RUN chmod +x /opt/spark/jobs/jobs/*.py

        # Create data directories
        RUN mkdir -p /data/ops /data/checkpoints /data/out

        USER 1001
        EOF

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Log in to Container Registry
      if: github.event_name != 'pull_request'
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}

    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/spark-job
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=semver,pattern={{version}}
          type=semver,pattern={{major}}.{{minor}}
          type=sha,prefix={{branch}}-

    - name: Build and push Docker image
      uses: docker/build-push-action@v5
      with:
        context: .
        file: ./Dockerfile.spark
        platforms: linux/amd64,linux/arm64
        push: ${{ github.event_name != 'pull_request' }}
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

  security-scan:
    name: Security Scan Images
    runs-on: ubuntu-latest
    needs: [build-producer, build-spark]
    if: github.event_name != 'pull_request'

    steps:
    - name: Run Trivy vulnerability scanner on Producer
      uses: aquasecurity/trivy-action@master
      with:
        image-ref: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/producer:${{ github.sha }}
        format: 'sarif'
        output: 'trivy-producer.sarif'

    - name: Run Trivy vulnerability scanner on Spark
      uses: aquasecurity/trivy-action@master
      with:
        image-ref: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/spark-job:${{ github.sha }}
        format: 'sarif'
        output: 'trivy-spark.sarif'

    - name: Upload Trivy scan results to GitHub Security tab
      uses: github/codeql-action/upload-sarif@v2
      if: always()
      with:
        sarif_file: 'trivy-producer.sarif'

    - name: Upload Trivy scan results to GitHub Security tab
      uses: github/codeql-action/upload-sarif@v2
      if: always()
      with:
        sarif_file: 'trivy-spark.sarif'

  image-size-check:
    name: Check Image Sizes
    runs-on: ubuntu-latest
    needs: [build-producer, build-spark]
    if: github.event_name == 'pull_request'

    steps:
    - name: Check Producer image size
      run: |
        docker pull ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/producer:pr-${{ github.event.number }} || echo "Image not found (expected for PRs)"
        # Add size checks here if needed

    - name: Comment PR with image info
      uses: actions/github-script@v6
      if: github.event_name == 'pull_request'
      with:
        script: |
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: '🐳 Docker images have been built successfully for this PR!\n\n' +
                  '**Images:**\n' +
                  `- Producer: \`${process.env.REGISTRY}/${process.env.IMAGE_NAME}/producer:pr-${context.issue.number}\`\n` +
                  `- Spark Job: \`${process.env.REGISTRY}/${process.env.IMAGE_NAME}/spark-job:pr-${context.issue.number}\`\n\n` +
                  'Images are available for testing and will be automatically removed after 7 days.'
          })

  cleanup-old-images:
    name: Cleanup Old Images
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
    - name: Delete old container images
      uses: actions/delete-package-versions@v5
      with:
        package-name: ${{ env.IMAGE_NAME }}/producer
        package-type: container
        min-versions-to-keep: 10
        delete-only-untagged-versions: true

    - name: Delete old spark images
      uses: actions/delete-package-versions@v5
      with:
        package-name: ${{ env.IMAGE_NAME }}/spark-job
        package-type: container
        min-versions-to-keep: 10
        delete-only-untagged-versions: true